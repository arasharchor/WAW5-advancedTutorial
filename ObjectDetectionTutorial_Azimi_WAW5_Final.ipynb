{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ObjectDetectionTutorial_Azimi_WAW5_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQAKz6BdBjCt",
        "colab_type": "text"
      },
      "source": [
        "LINK TO THE NOTEBOOK:\n",
        "\n",
        "[shorturl.at/aQSTY](shorturl.at/aQSTY)\n",
        "\n",
        "Seyed Majid Azimi, DLR, IMF-PBA\n",
        "seyedmajid.azimi@dlr.de\n",
        "\n",
        "# Object Detection Tutorial using Deep Learning Methods with PyTorch\n",
        "#### honorable mentions and credit: Machine-Vision Research Group @Fractal Analytics, Ross Girschick, Kaiming He, Detectron2\n",
        "\n",
        "For this tutorial, we will be explaining the recent milestone at another computer vision application called object detection. We will explain the cores of recent algorithms and the implenetation of some parts. In the end, we will apply one of them on several test images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtTyOjDOLw16",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification vs. Object Detection\n",
        "\n",
        "\n",
        "Image Classification is a problem where we assign a class label to an input image. For example, given an input image of a cat, the output of an image classification algorithm is the label “Cat”.\n",
        "\n",
        "In object detection, we are not only interested in what objects are in the input image, but we are also interested in where they are located.\n",
        "\n",
        "![imageclassificationvsobjectdetection](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/image-classification-vs-object-detection.png)\n",
        "\n",
        "The figure above illustrates the difference between image classification and object detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH_d9VMsL2HF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 1.1. Image Classification vs Object Detection : Which one to use?\n",
        "\n",
        "\n",
        "In most applications where there are more than one object in the input image, we need to find the location of the objects, and then classify them. We use an object detection algorithm in such case.\n",
        "\n",
        "Object detection can be hundreds of times slower than image classification, and therefore, in applications where the location of the object in the image is not important, we use image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhkx4RLDzGN7",
        "colab_type": "text"
      },
      "source": [
        "# Object Detection\n",
        "\n",
        "The object detection task is composed of two steps, object localization and object classification. \n",
        "\n",
        "Object Localization is to draw a bounding box around the region of the object in the image and object localization is to classify the object inside that bounding box. So basically the output of the object detection algorithm is 4 floating values of the coordinates of each bounding box andd the corresponding class from a list of known classes.\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "## Sliding Window Approach\n",
        "Traditionally object-detection was implemented using simple template matching techniques. In these methods, target objects used to be cropped and using specific descriptors like HOG and SIFT, features for the same used to be generated. The approach subsequently used a sliding-window on the image and compared each location with the database of object feature vectors. Enhanced algorithms have used classifiers, like trained SVM classifiers to replace the use of these databases. Since objects will be of different sizes, people used different window sizes and image sizes (Image Pyramids). These complex pipelines managed partly solved the object-detection problem, but had many drawbacks. The pipelines were computationally time consuming, and the hand engineered features using algorithms like HOG and SIFT were not highly accurate.\n",
        "\n",
        "\n",
        "\n",
        "## Region Proposals with CNN (R-CNN)\n",
        "With the advent of the use of deep learning in machine-vision and the staggering results these algorithms got for image classification challenges in 2012, researchers started looking at deep-learning solutions to solve object detection problems. One of the first important algorithms to solve object detection using deep learning was R-CNN ( Region proposals with CNN)\n",
        "\n",
        "This is the first paper to show that CNN can lead to dramatically higher Object detection performance. It achieved 58.5 mAP on VOC 2007 data-set.\n",
        "\n",
        "In R-CNN an image processing technique is used to make list of proposed regions in the input image which are then sent through the network for classification. But this is computationally more efficient than sliding window approach as only fewer potential crops which may contain the object is classified by the network.\n",
        "  \n",
        "  ![R-CNN](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/RCNN.png)\n",
        "\n",
        "  Image Source :  [Ross Girshick et al](https://arxiv.org/pdf/1311.2524.pdf)\n",
        "\n",
        "\n",
        "To build an R-CNN object-detection pipeline, the following approach can be used\n",
        "\n",
        "    The approach starts with a standard network like VGG or ResNet pre-trained on Image-net — this network will act like a feature extractor for the image. The approach removes the class specific classification layer and uses the bottleneck layer to extract features from the image. In R-CNN the approach has used VGG network and we get a 4096-dim vector for each image proposal.\n",
        "\n",
        "![RCNN WARPED OBJECTS](https://miro.medium.com/proxy/1*0S3dZGri0zAbt4tFSn5k0w.png)\n",
        "\n",
        "    they train the network using these warped images. While training a dataset such as Pascal VOC, they place 20+1 (n_class + background) layer at the end and train the network. Each batch size contains 32 positive windows and 96 background windows. A Region proposal is said to be positive if it has an IoU ≥ 0.5 with the ground truth box. Usually obtain these 128 (32+96) proposals from 2 images (batch size). Since there will be ~2k proposals from each image, we sample the positive images and negative images(background) images separately. We use selective search to generate these proposals.\n",
        "    After fine-tuning the network, we will send the proposals through the network and obtain a 4096 dim vector. The authors of this paper performed a grid search on IOU to select the positive samples, IOU of 0.3 worked well for them.\n",
        "    For each object class, train a SVM (one versus other) classifier. You can use hard negative mining to improve the classification accuracy.\n",
        "    We also train a bounding box regressors to improve upon the localization errors. This is applied on the proposal once the class specific SVM classifier classify the object.\n",
        "    \n",
        "    \n",
        "Testing the algorithm\n",
        "\n",
        "    For testing, R-CNN generates around 2000 category-independent region proposal from the input image, extracts a fixed-length feature vector for each proposal using a CNN (VGG Net), and then classifies each region with category specific linear SVM’s. This gives class specific ‘objectness’ score for each proposal, which are then sent a non-maxim suppression algorithm.\n",
        "    Test time inference takes 13 sec per image on GPU and 53s/image on CPU.\n",
        "\n",
        "The major problem with this approach are :\n",
        "\n",
        "    Sending ~2000 proposals to the Neural network, thus bringing test time to 13 sec on GPU.\n",
        "    Complex pipeline for training and inference. No end to end training pipeline. Neural network is trained separately, SVM classifiers are trained individually.\n",
        "    \n",
        "    \n",
        "  RCNN is better than sliding window, but its still computationally expensive as the network has to classify all the region proposals. It takes around 30-40s for inference of a single image.\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBGjqGd1G5L9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "  \n",
        "- ## Fast Region Proposal (Fast R-CNN)\n",
        "\n",
        "This paper provided break-through results and has set the standards for the approaches that followed it. Major differences brought by Fast R-CNN:\n",
        "\n",
        "![Fast R-CNN](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/Fast-RCNN.png)\n",
        "  Image Source : [Ross Gishick](https://arxiv.org/pdf/1504.08083.pdf)\n",
        "  \n",
        "*    Removed SVM classifiers and joined a regression and classification layer to the net with a multi-task loss function. Thus making it a single stage training and inference network.\n",
        "*    Remove 2000 CNN execution for each object proposal and joined it with ROI pooling layer which runs once.\n",
        "*    Use VGGNet as the backend rather than AlexNet.\n",
        "*    Build new loss functions which are less sensitive to outliers.\n",
        "\n",
        "Fast R-CNN network works in the following way\n",
        "\n",
        "    The network processes the whole image to produce a convolutional feature map. Then for each object proposal ROI pooling layer extracts fixed-length feature vector, which is finally passed to subsequent FC layers.\n",
        "    The FC layers branch into two sibling output layers, one that estimates softmax probability over K+1 object classes, and another layer producing the refined bounding box positions.\n",
        "    2000 proposals of particular image are not passed through the network as in R-CNN, Instead, The image is passed only once and the computed features are shared across ~2000 proposals like the same way it is done in SPP Net .\n",
        "    Also, the ROI pooling layer does max pooling in each sub-window of approximate size h/H X w/W. H and W are hyper-parameters. It is a special case of SPP layer with one pyramid level.\n",
        "    The two sibling output layers’ outputs are used to calculate a multi-task loss on each labeled ROI to jointly train for classification and bounding-box regression.\n",
        "    They have used L1 loss for bounding box regression as opposed to L2 loss in R-CNN and SPP-Net which is more sensitive to outliers.\n",
        "    \n",
        "    \n",
        "In short, in fast R-CNN, rather than getting region proposals and classifying each region proposals separately, the input image is sent into the CNN network which gives a feature map of the image. Again some region proposals are used, but now we get the region proposals from the feature map of the image and these feature maps are classified. This reduces the computation as some of the CNN layers are common for the whole image. \n",
        "\n",
        "Credit to [SPP](https://arxiv.org/abs/1512.02325) for the improvement on R-CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T9uYwQAGqck",
        "colab_type": "text"
      },
      "source": [
        "### RoI Pooling Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dYTcXZSGuKs",
        "colab_type": "code",
        "outputId": "a8008d77-527f-41ba-9531-b292ad2d3c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-1939fe950330>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRoIPoolFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooled_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooled_width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooled_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooled_height\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Function' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TvfW97THPUI",
        "colab_type": "text"
      },
      "source": [
        "## Multi-task Loss Function\n",
        "\n",
        "## Loss Functions\n",
        "\n",
        "Regression uses smooth L1 loss, which is less sensitive to outliers. This is the same loss used in all the frameworks till now.\n",
        "\n",
        "![smooth L1 loss](https://miro.medium.com/proxy/1*ct5e8rEJYIK4SJxPTYEFWA.png)\n",
        "\n",
        "\n",
        "Classification loss is called focal loss, which is a reshaped version of cross entropy loss and this paper talks a lot about this.\n",
        "\n",
        "Lets look at how this focal loss is designed. We will first look at binary cross entropy loss for single object classification\n",
        "\n",
        "Cross entropy loss:\n",
        "\n",
        "<center>\n",
        "  \n",
        "![cross-entropy](https://miro.medium.com/proxy/1*0bxc7T4lRrcRlYIFeiIbeg.png)\n",
        "  \n",
        "</center>\n",
        "\n",
        "further advanced information:\n",
        "\n",
        "Focal loss:\n",
        "\n",
        "<center>\n",
        "\n",
        "  ![Focal loss](https://miro.medium.com/proxy/1*iD5yJGfE_odwUaYEgAqOrA.png)\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "*    where p_{t} is p if y=1 and 1-p otherwise. Lets look at the focal loss equation clearly. Here alpha is called the balancing param and gamma is called the focusing param.\n",
        "\n",
        "### Final step: \n",
        "1000 proposals from each feature scale were obtained after thresholding detector confidence at 0.05. The top predictions from all levels are merged and non-maximum suppression with a threshold of 0.5 is applied to yield the final detections.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xGO6tQOHOcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Smooth L1 Loss\n",
        "def _smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):\n",
        "\n",
        "    sigma_2 = sigma ** 2\n",
        "    box_diff = bbox_pred - bbox_targets\n",
        "    in_box_diff = bbox_inside_weights * box_diff\n",
        "    abs_in_box_diff = torch.abs(in_box_diff)\n",
        "    \n",
        "    \n",
        "    smoothL1_sign = (abs_in_box_diff < 1. / sigma_2).detach().float()\n",
        "    \n",
        "    in_loss_box = torch.pow(in_box_diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\n",
        "                  + (abs_in_box_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n",
        "    \n",
        "    \n",
        "    out_loss_box = bbox_outside_weights * in_loss_box\n",
        "    loss_box = out_loss_box\n",
        "\n",
        "    s = loss_box.size(0)\n",
        "    loss_box = loss_box.view(s, -1).sum(1).mean()\n",
        "    # for i in sorted(dim, reverse=True):\n",
        "    #   loss_box = loss_box.sum(i)\n",
        "    # loss_box = loss_box.mean()\n",
        "    return loss_box\n",
        "\n",
        "# ROI Network\n",
        "# loss (cross entropy) for object classification\n",
        "RCNN_loss_cls = F.cross_entropy(cls_score, rois_label)\n",
        "\n",
        "# loss (l1-norm) for bounding box regression\n",
        "RCNN_loss_bbox = _smooth_l1_loss(bbox_pred, rois_target, rois_inside_ws, rois_outside_ws)\n",
        "\n",
        "\n",
        "Fast_RCNN_loss = RCNN_loss_cls.mean() + RCNN_loss_bbox.mean()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DZgfTZKPlzm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "  \n",
        "\n",
        "## Faster R-CNN\n",
        "\n",
        "This approach is still the best choice for most of the researchers and has achieved incredible results. The algorithm has surpassed all the previous results in-terms of both accuracy and speed. Faster R-CNN also has come up with new techniques which have become gold standards for all upcoming frameworks. Lets have deep look into these methods.\n",
        "\n",
        "Changes to the Fast R-CNN\n",
        "\n",
        "*    Removed Selective search and added a Deep Neural network for generating proposals (RPN network)\n",
        "*    Introduced anchor boxes\n",
        "\n",
        "Anchor boxes became very common from here for all the frameworks. RPN network can work as single object detector or generate proposals for Fast R-CNN network. One thing is for sure, we have removed the traditional computer vision techniques completely and made a full fledged deep neural network which gets trained end to end.\n",
        "\n",
        "\n",
        "  The idea of Faster R-CNN is to use CNNs to propose potential region of interest and the network is called Region Proposal Network. After getting the region proposals , its just like Fast RCNN, we use every regions for classification.\n",
        "![Faster R-CNN](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/Faster-RCNN.png)\n",
        "\n",
        "![Anchor](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/anchor.png)\n",
        "Image Source : [Shaoqing Ren et al](https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf)\n",
        "\n",
        "# Putting all together\n",
        "![-Fast-er- R-CNN FPN](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/whole2.jpg)\n",
        "\n",
        "##Overall procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0eG98dY6IbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. getting input arguments for Pascal VOC dataset\n",
        "args = parse_args()\n",
        "# 2. getting VOC dataset names and region of interests and image database.\n",
        "imdb, roidb, ratio_list, ratio_index = combined_roidb(args.imdb_name)\n",
        "# 3. \n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size,                            \t\t\t\t\t\t\t\t\t\tsampler=sampler_batch, num_workers=args.num_workers)\n",
        "# 4. faster rcnn backbone of ResNet101\n",
        "fasterRCNN = resnet(imdb.classes, 101, pretrained=True, class_agnostic=args.class_agnostic)\n",
        "# 5. rcnn\n",
        "for epoch in range(args.start_epoch, args.max_epochs + 1):\n",
        "    # 5.1 \n",
        "    fasterRCNN.train()\n",
        "    # 5.2 \n",
        "    if epoch % (args.lr_decay_step + 1) == 0:\n",
        "        adjust_learning_rate(optimizer, args.lr_decay_gamma)\n",
        "        lr *= args.lr_decay_gamma\n",
        "    # 5.3    \n",
        "   \tfor data in dataloaer:\n",
        "        # 5.3.1 \n",
        "        im_data.resize_(data[0].size()).copy_(data[0])\n",
        "        im_info.resize_(data[1].size()).copy_(data[1])\n",
        "        gt_boxes.resize_(data[2].size()).copy_(data[2])\n",
        "        num_boxes.resize_(data[3].size()).copy_(data[3])\n",
        "        \n",
        "        fasterRCNN.zero_grad()\n",
        "        # 5.3.2 \n",
        "        rois, cls_prob, bbox_pred, \\\n",
        "      \trpn_loss_cls, rpn_loss_box, \\\n",
        "      \tRCNN_loss_cls, RCNN_loss_bbox, \\\n",
        "      \trois_label = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)\n",
        "        # 5.3.3 \n",
        "        loss = rpn_loss_cls.mean() + rpn_loss_box.mean() \\\n",
        "           \t+ RCNN_loss_cls.mean() + RCNN_loss_bbox.mean()\n",
        "        # 5.3.4      \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def combined_roidb(imdb_names, training=True):\n",
        "    roidbs = [get_roidb(s) for s in imdb_names.split('+')]\n",
        "  \troidb = roidbs[0] # len(roidb) = 1002\n",
        "  \t\n",
        "    if len(roidb) > 1:\n",
        "        pass\n",
        "    else:\n",
        "\t\t  imdb = get_imdb(imdb_names)\n",
        "        \n",
        "    if training:\n",
        "      roidb = filter_roidb(roidb)\n",
        "        \n",
        "    ratio_list, ratio_index = rank_roidb_ratio(roidb)\n",
        "    \n",
        "    return imdb, roidb, ratio_list, ratio_index\n",
        "\n",
        "def get_roidb(imdb_name):\n",
        "    imdb = get_imdb(imdb_name)\n",
        "\n",
        "    imdb.set_proposal_method(cfg.TRAIN.PROPOSAL_METHOD)\n",
        "\n",
        "    roidb = get_training_roidb(imdb)\n",
        "    return roidb\n",
        "\n",
        "def get_imdb(name):\n",
        "    return __sets[name]()\n",
        "    #>>> imdb: <datasets.pascal_voc.pascal_voc object at 0x7f60e737d860>\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLEyutRN411f",
        "colab_type": "text"
      },
      "source": [
        "## Faster RCNN Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxukL5Ub4r5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RPN\n",
        "rois, rpn_loss_cls, rpn_loss_bbox \n",
        "\t= self.RCNN_rpn(base_feat, im_info, gt_boxes, num_boxes)\n",
        "\n",
        "self.rpn_loss_cls = F.cross_entropy(rpn_cls_score, rpn_label)\n",
        "self.rpn_loss_box = _smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, \n",
        "                                    rpn_bbox_inside_weights,\n",
        "                                    rpn_bbox_outside_weights, sigma=3, dim=[1,2,3])\n",
        "\n",
        "\n",
        "Faster_RCNN_loss = rpn_loss_cls.mean() + rpn_loss_box.mean() \\\n",
        "           + RCNN_loss_cls.mean() + RCNN_loss_bbox.mean()  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qJythXY8Ap3",
        "colab_type": "text"
      },
      "source": [
        "## Region Proposal Network (RPN)\n",
        "  Take an input image of 800x800 and send it to the network. For a VGG Network, after subsampling ratio of 16, the output will be [512, 50, 50]. An RPN network is applied on this feature map, which generates (50*50*9) boxes regression and classification scores. So regression output is 50*50*9*4 (x,y, w, h) and classification output is 50*50*9*2 (object present or not). Here 9 implies the number of anchor boxes at each location. Below is the procedure on how anchor boxes are generated on the image. (Note: With slight modifications, this is how anchor boxes are applied mostly on all the following frameworks)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfftQps479xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKJB7s7Y7-Lu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "    \n",
        "Anchor boxes\n",
        "\n",
        "  At each location, we have 9 anchor boxes, and they have different scales and aspect ratios. The following are defined\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxOeHC2QMRE3",
        "colab_type": "text"
      },
      "source": [
        "## Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1RzgFFi8IS_",
        "colab_type": "code",
        "outputId": "cba7ec2d-274d-4ddb-80c0-0661028f9fa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "req_features = []\n",
        "k = dummy_img.clone()\n",
        "for i in fe:\n",
        "    k = i(k)\n",
        "    if k.size()[2] < 800//16:\n",
        "        break\n",
        "    req_features.append(i)\n",
        "    out_channels = k.size()[1]\n",
        "\n",
        "print(len(req_features)) #7\n",
        "print(out_channels) # 256\n",
        "\n",
        "faster_rcnn_fe_extractor = nn.Sequential(*req_features)\n",
        "\n",
        "out_map = faster_rcnn_fe_extractor(dummy_img)\n",
        "print(out_map.size()) #Out: torch.Size([1, 256, 50, 50])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "256\n",
            "torch.Size([1, 256, 50, 50])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWvLAqQNMPrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anchors_boxes_per_location = 9\n",
        "scales = [8, 16, 32]\n",
        "ratios = [0.5, 1, 2]\n",
        "ctr_x, ctr_y = 16/2, 16/2 #(at (1,1) location)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A0iFgw1zP4e",
        "colab_type": "text"
      },
      "source": [
        "At feature map location 1,1 is mapped to [0, 0, 16, 16] box on the image. this has center at 8, 8. Now we need to draw the 9 anchor boxes using the above scales and ratios. A look at all the centers on the image\n",
        "\n",
        "![anchor points](https://miro.medium.com/proxy/1*f-AxsYA9ys5wtiY9NDZh9Q.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjMZOsgH0RXV",
        "colab_type": "code",
        "outputId": "d06e858f-ca0c-4274-f5d5-4cb99b58905f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "anchor_base = np.zeros((len(ratios) * len(scales), 4), dtype=np.float32)\n",
        "print(anchor_base)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC0e0AiV0Vfs",
        "colab_type": "code",
        "outputId": "e1e10447-74be-466f-cded-e13b98a70b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "sub_sample = 16\n",
        "\n",
        "for i in range(len(ratios)):\n",
        "  for j in range(len(scales)):\n",
        "    h = sub_sample * scales[j] * np.sqrt(ratios[i])\n",
        "    w = sub_sample * scales[j] * np.sqrt(1./ ratios[i])\n",
        "    index = i * len(scales) + j\n",
        "    anchor_base[index, 0] = ctr_y - h / 2.\n",
        "    anchor_base[index, 1] = ctr_x - w / 2.\n",
        "    anchor_base[index, 2] = ctr_y + h / 2.\n",
        "    anchor_base[index, 3] = ctr_x + w / 2.\n",
        "    \n",
        "print(anchor_base)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ -37.254833  -82.50967    53.254833   98.50967 ]\n",
            " [ -82.50967  -173.01933    98.50967   189.01933 ]\n",
            " [-173.01933  -354.03867   189.01933   370.03867 ]\n",
            " [ -56.        -56.         72.         72.      ]\n",
            " [-120.       -120.        136.        136.      ]\n",
            " [-248.       -248.        264.        264.      ]\n",
            " [ -82.50967   -37.254833   98.50967    53.254833]\n",
            " [-173.01933   -82.50967   189.01933    98.50967 ]\n",
            " [-354.03867  -173.01933   370.03867   189.01933 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1fSNG8624zU",
        "colab_type": "text"
      },
      "source": [
        "This is for one location, now we need to apply for all the anchor centers. since there are 50*50 anchor centers and each one has 9 anchors. in total we get 22500 anchors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGJjtZWR23RL",
        "colab_type": "code",
        "outputId": "2589bac5-4ea1-4d20-b8ca-9a2fa40fa400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fe_size = 50\n",
        "anchors = np.zeros((fe_size * fe_size * 9, 4))\n",
        "index = 0\n",
        "for i in range(len(ratios)):\n",
        "  for j in range(len(scales)):\n",
        "    h = sub_sample * scales[j] * np.sqrt(ratios[i])\n",
        "    w = sub_sample * scales[j] * np.sqrt(1./ ratios[i])\n",
        "    anchors[index, 0] = ctr_y - h / 2.\n",
        "    anchors[index, 1] = ctr_x - w / 2.\n",
        "    anchors[index, 2] = ctr_y + h / 2.\n",
        "    anchors[index, 3] = ctr_x + w / 2.\n",
        "    index += 1\n",
        "\n",
        "print(anchors.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(22500, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbIl0wrSRCay",
        "colab_type": "text"
      },
      "source": [
        "The final two matrices are\n",
        "\n",
        "    anchor_locations [N, 4] — [22500, 4]\n",
        "    anchor_labels [N,] — [22500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPofPy7d4MZd",
        "colab_type": "text"
      },
      "source": [
        "A look at anchors at (400, 400) on image\n",
        "\n",
        "![anchors](https://miro.medium.com/proxy/1*cPidpSRVUVgv3YeY9Fc11Q.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzL_L_FO45Qh",
        "colab_type": "text"
      },
      "source": [
        "*   So the RPN produces object proposals wrt to each anchor box along with their objectness score. An anchor box is assigned as positive if it has max_iou with the ground truth object or iou greater than 0.7. An anchor box is assigned negative if it has iou < 0.4. All the anchor boxes with iou [0.4, 0.7] are ignored. Anchor boxes which fall outside the image are also ignored.\n",
        "*   Again, since vast majority of them will have negative samples, we will use the same Fast R-CNN strategy to sample 128+ samples and 128- samples (total 256) for a batch size of 2 for training.\n",
        "\n",
        "*  Smooth L1 loss and cross entropy loss can be used for regression and classification. Regression outputs are offset with anchor box locations using the following formula\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA5E3VqG5yd_",
        "colab_type": "text"
      },
      "source": [
        "t_{x} = (x - x_{a})/w_{a}\n",
        "\n",
        "t_{y} = (y - y_{a})/h_{a}\n",
        "\n",
        "t_{w} = log(w/ w_a)\n",
        "\n",
        "t_{h} = log(h/ h_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7L7vCMP6Anx",
        "colab_type": "text"
      },
      "source": [
        "x, y , w, h are the ground truth box center co-ordinates, width and height. x_a, y_a, h_a and w_a and anchor boxes center cooridinates, width and height.\n",
        "\n",
        "*    Once RPN outputs are generated, we need to process them before sending to the RoI pooling layer (aka fast R-CNN network) . The Faster R_CNN says, RPN proposals highly overlap with each other. To reduced redundancy, we adopt non-maximum supression (NMS) on the proposal regions based on their cls scores. We fix the IoU threshold for NMS at 0.7, which leaves us about 2000 proposal regions per image. After an ablation study, the authors show that NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals. After NMS, we use the top-N ranked proposal regions for detection. In the following we training Fast R-CNN using 2000 RPN proposals. During testing they evaluate only 300 proposals, they have tested this with various numbers and obtained this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xObe_wBE5tgQ"
      },
      "source": [
        "Once the ~2000 proposals are generated, these are transformed using the following formulas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk6fcFhn6eg2",
        "colab_type": "text"
      },
      "source": [
        "x = (w_{a} * ctr_x_{p}) + ctr_x_{a}\n",
        "\n",
        "y = (h_{a} * ctr_x_{p}) + ctr_x_{a}\n",
        "\n",
        "h = np.exp(h_{p}) * h_{a}\n",
        "\n",
        "w = np.exp(w_{p}) * w_{a}\n",
        "\n",
        "and later convert to y1, x1, y2, x2 format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIgaVNUOzBLH",
        "colab_type": "text"
      },
      "source": [
        "## Feature Pyramid Networks\n",
        "\n",
        "In this line, Researchers have observed two problems with the Faster R-CNN. First it is unable to detect small objects, second class imbalance is not focussed properly (Random sampling 256 samples and calculating loss is not a proper way). So the researches have introduced two new concepts\n",
        "\n",
        "*   Feature Pyramid networks [FPN](https://arxiv.org/abs/1612.03144)  \n",
        "\n",
        "The idea of FPN is to process the images in multi-scale fashion rather than one single scale.\n",
        "\n",
        "\n",
        "![Faster R-CNN FPN](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/FPN-1.png)\n",
        "\n",
        "\n",
        "Image Source:[Tsung-Yi Lin et al](https://arxiv.org/abs/1612.03144)\n",
        " \n",
        "Faster RCNN is mostly unable to catch small objects in the image. This is largely addressed in COCO and ILSVRC challege using image pyramids by most of the winning teams. A simple image pyramid is given below, you scale image to different sizes and send it to the network, Once the detections are detected on each scale, all the predictions are combined using different method. Though this method worked, Inference is a costly process as each image should be computed at various scales independently.\n",
        "\n",
        "\n",
        "A deep ConvNet computes a feature hierarchy layer by layer, and with sub-sampling layers the feature hierarchy has an inherent multi-scale, pyramidal shape. This in-network feature hierarchy produces feature maps of different spatial resolutions. First lets see how FPN works and later we will move into the intuition part.\n",
        "\n",
        "*    First take a standard resnet architecture(Ex ResNet50). In Faster R-CNN, discussed above, we have considered only feature maps of sub-sampling ratio 16 were taken to compute Region proposals and later pass the RPN outputs to Fast RCNN. Here it is done in the following way.\n",
        "\n",
        "![FPN-module2](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/fpn-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZooJbaDW82Pa",
        "colab_type": "text"
      },
      "source": [
        "### ResNet Backbone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGpltLOv85aL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## For information - do not execute\n",
        "      \n",
        "class resnet_backbone(nn.Module):\n",
        "    def __init__(self, num_class = 1, input_channel = 3, output_stride=32, layer=101):\n",
        "        super(resnet_backbone, self).__init__()\n",
        "        if layer == 101:\n",
        "            self.resnet = resnet101(pretrained=True, output_stride=output_stride)\n",
        "        elif layer == 152:\n",
        "            self.resnet = resnet152(pretrained=True, output_stride=output_stride)\n",
        "        elif layer == 50:\n",
        "            self.resnet = resnet50(pretrained=True, output_stride=output_stride)\n",
        "        else:\n",
        "            raise ValueError(\"only support ResNet101 and ResNet152 now\")\n",
        "\n",
        "        if input_channel == 1:\n",
        "            self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding = 3, bias=False)\n",
        "        elif input_channel == 3:\n",
        "            self.conv1 = self.resnet.conv1\n",
        "        else:\n",
        "            raise ValueError(\"input channel should be 3 or 1\")\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        c1 = self.conv1(x) #1, 320*320\n",
        "        c1 = self.resnet.bn1(c1)\n",
        "        c1 = self.resnet.relu(c1)\n",
        "        c1 = self.resnet.maxpool(c1) #4, 80*80\n",
        "\n",
        "        c2 = self.resnet.layer1(c1)\n",
        "        c3 = self.resnet.layer2(c2) #8, 40*40\n",
        "        c4 = self.resnet.layer3(c3) #16, 20*20\n",
        "        c5 = self.resnet.layer4(c4) #32, 10*10\n",
        "\n",
        "        return c1, c2, c3, c4, c5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmgNduLb9EeL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*    The problem with excessive sub sampling is it will lead to poor localization. If incase we want to use features from earlier layers (say subsample 8) the semantics of the object are not captured very clearly. So, we have to take the best of both the worlds. With this intuition feature pyramid networks or FPN is designed first on Faster RCNN. The latlayers reduces the channel dimension (number of feature maps), up-sampled and added to the previous layer outputs. Since up-sampling is done using bilinear interpolation, they have added another conv layer this output so that the aliasing effect of up-sampling is removed. We have ignored p1 because of the computational complexity (It generates (400*400*3 = 480k proposals for a single feature map p1).\n",
        "\n",
        "![FPN-module](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/fpn-2.png)\n",
        "\n",
        "\n",
        "*    Anchor boxes are designed on each feature map separately. Since scale is taken care off by the FPN, At each location we take anchors of 3 aspect ratio [1:2, 2:1, 1:1]. In total we get 9 anchors at each location over the feature map scale.\n",
        "\n",
        "\n",
        "#### Results:\n",
        "\n",
        "*    some ablation studies suggest that using FPN, Average precision (AR) has improved by 8.0 points over a single scale RPN.\n",
        "*    lateral connections improved the AP by 10 points.\n",
        "*    On small objects the coco style AP has improved from 9.6% to 17.8%.\n",
        "*    Inference on each image takes 176 ms when using resnet101 as the backend.\n",
        "    \n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-rkyZ5E9Zmr",
        "colab_type": "text"
      },
      "source": [
        "### FPN Base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbS2bgAI9RAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## For information - do not execute\n",
        "\n",
        "def _upsample_add(self, x, y):\n",
        "        '''Upsample and add two feature maps.\n",
        "        Args:\n",
        "          x: (Variable) top feature map to be upsampled.\n",
        "          y: (Variable) lateral feature map.\n",
        "        Returns:\n",
        "          (Variable) added feature map.\n",
        "        '''\n",
        "        _,_,H,W = y.size()\n",
        "        return F.upsample(x, size=(H,W), mode='bilinear') + y\n",
        "\n",
        "\n",
        "class _FPN(nn.Module):\n",
        "    \"\"\" FPN \"\"\"\n",
        "    def __init__(self, classes, class_agnostic):\n",
        "        super(_FPN, self).__init__()\n",
        "        self.classes = classes\n",
        "        self.n_classes = len(classes)\n",
        "        self.class_agnostic = class_agnostic\n",
        "        # loss\n",
        "        self.RCNN_loss_cls = 0\n",
        "        self.RCNN_loss_bbox = 0\n",
        "\n",
        "        self.maxpool2d = nn.MaxPool2d(1, stride=2)\n",
        "        # define rpn\n",
        "        self.RCNN_rpn = _RPN_FPN(self.dout_base_model)\n",
        "        self.RCNN_proposal_target = _ProposalTargetLayer(self.n_classes)\n",
        "\n",
        "        # NOTE: the original paper used pool_size = 7 for cls branch, and 14 for mask branch, to save the\n",
        "        # computation time, we first use 14 as the pool_size, and then do stride=2 pooling for cls branch.\n",
        "        self.RCNN_roi_pool = _RoIPooling(cfg.POOLING_SIZE, cfg.POOLING_SIZE, 1.0/16.0)\n",
        "        self.RCNN_roi_align = RoIAlignAvg(cfg.POOLING_SIZE, cfg.POOLING_SIZE, 1.0/16.0)\n",
        "        self.grid_size = cfg.POOLING_SIZE * 2 if cfg.CROP_RESIZE_WITH_MAX_POOL else cfg.POOLING_SIZE\n",
        "        self.RCNN_roi_crop = _RoICrop()\n",
        "        \n",
        "      def forward(self, im_data, im_info, gt_boxes, num_boxes):\n",
        "        batch_size = im_data.size(0)\n",
        "\n",
        "        im_info = im_info.data\n",
        "        gt_boxes = gt_boxes.data\n",
        "        num_boxes = num_boxes.data\n",
        "\n",
        "        # feed image data to base model to obtain base feature map\n",
        "        # Bottom-up\n",
        "        c1 = self.RCNN_layer0(im_data)\n",
        "        c2 = self.RCNN_layer1(c1)\n",
        "        c3 = self.RCNN_layer2(c2)\n",
        "        c4 = self.RCNN_layer3(c3)\n",
        "        c5 = self.RCNN_layer4(c4)\n",
        "        # Top-down\n",
        "        p5 = self.RCNN_toplayer(c5)\n",
        "        p4 = self._upsample_add(p5, self.RCNN_latlayer1(c4))\n",
        "        p4 = self.RCNN_smooth1(p4)\n",
        "        p3 = self._upsample_add(p4, self.RCNN_latlayer2(c3))\n",
        "        p3 = self.RCNN_smooth2(p3)\n",
        "        p2 = self._upsample_add(p3, self.RCNN_latlayer3(c2))\n",
        "        p2 = self.RCNN_smooth3(p2)\n",
        "\n",
        "        p6 = self.maxpool2d(p5)\n",
        "\n",
        "        rpn_feature_maps = [p2, p3, p4, p5, p6]\n",
        "        mrcnn_feature_maps = [p2, p3, p4, p5]\n",
        "\n",
        "        rois, rpn_loss_cls, rpn_loss_bbox = self.RCNN_rpn(rpn_feature_maps, im_info, gt_boxes, num_boxes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFsLRVfcR5BF",
        "colab_type": "code",
        "outputId": "66261769-78df-420d-9c17-a791fc952a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# import necessary libraries\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# get the pretrained model from torchvision.models\n",
        "# Note: pretrained=True will get the pretrained weights for the model.\n",
        "# model.eval() to use the model for inference\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Class labels from official PyTorch documentation for the pretrained model\n",
        "# Note that there are some N/A's \n",
        "# for complete list check https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
        "# we will use the same list for this notebook\n",
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "\n",
        "def get_prediction(img_path, threshold):\n",
        "  \"\"\"\n",
        "  get_prediction\n",
        "    parameters:\n",
        "      - img_path - path of the input image\n",
        "      - threshold - threshold value for prediction score\n",
        "    method:\n",
        "      - Image is obtained from the image path\n",
        "      - the image is converted to image tensor using PyTorch's Transforms\n",
        "      - image is passed through the model to get the predictions\n",
        "      - class, box coordinates are obtained, but only prediction score > threshold\n",
        "        are chosen.\n",
        "    \n",
        "  \"\"\"\n",
        "  img = Image.open(img_path)\n",
        "  transform = T.Compose([T.ToTensor()])\n",
        "  img = transform(img)\n",
        "  pred = model([img])\n",
        "  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
        "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
        "  pred_score = list(pred[0]['scores'].detach().numpy())\n",
        "  pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]\n",
        "  pred_boxes = pred_boxes[:pred_t+1]\n",
        "  pred_class = pred_class[:pred_t+1]\n",
        "  return pred_boxes, pred_class\n",
        "  \n",
        "\n",
        "\n",
        "def object_detection_api(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n",
        "  \"\"\"\n",
        "  object_detection_api\n",
        "    parameters:\n",
        "      - img_path - path of the input image\n",
        "      - threshold - threshold value for prediction score\n",
        "      - rect_th - thickness of bounding box\n",
        "      - text_size - size of the class label text\n",
        "      - text_th - thichness of the text\n",
        "    method:\n",
        "      - prediction is obtained from get_prediction method\n",
        "      - for each prediction, bounding box is drawn and text is written \n",
        "        with opencv\n",
        "      - the final image is displayed\n",
        "  \"\"\"\n",
        "  boxes, pred_cls = get_prediction(img_path, threshold)\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  for i in range(len(boxes)):\n",
        "    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(255, 255, 0), thickness=rect_th)\n",
        "    cv2.putText(img,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,255),thickness=text_th)\n",
        "  plt.figure(figsize=(20,30))\n",
        "  plt.imshow(img)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 167502836/167502836 [00:02<00:00, 59903861.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqCJnXHQSERm",
        "colab_type": "code",
        "outputId": "e3ff5a59-63e5-4c48-e77d-062451bec6b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# download an image for inference\n",
        "!wget https://www.wsha.org/wp-content/uploads/banner-diverse-group-of-people-2.jpg -O people.jpg\n",
        "\n",
        "# use the api pipeline for object detection\n",
        "# the threshold is set manually, the model sometimes predict \n",
        "# random structures as some object, so we set a threshold to filter\n",
        "# better prediction scores.\n",
        "object_detection_api('./people.jpg', threshold=0.8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-24 20:47:02--  https://www.wsha.org/wp-content/uploads/banner-diverse-group-of-people-2.jpg\n",
            "Resolving www.wsha.org (www.wsha.org)... 104.198.7.33\n",
            "Connecting to www.wsha.org (www.wsha.org)|104.198.7.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1923610 (1.8M) [image/jpeg]\n",
            "Saving to: ‘people.jpg’\n",
            "\n",
            "\rpeople.jpg            0%[                    ]       0  --.-KB/s               \rpeople.jpg           35%[======>             ] 663.65K  3.05MB/s               \rpeople.jpg          100%[===================>]   1.83M  5.77MB/s    in 0.3s    \n",
            "\n",
            "2019-09-24 20:47:02 (5.77 MB/s) - ‘people.jpg’ saved [1923610/1923610]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrdTMEeASDkN",
        "colab_type": "text"
      },
      "source": [
        "Let's try one more complex example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63VhMMimSNXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://cdn.pixabay.com/photo/2013/07/05/01/08/traffic-143391_960_720.jpg -O traffic.jpg\n",
        "\n",
        "object_detection_api('/content/traffic.jpg', rect_th=2, text_th=1, text_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiBGBOOSSbqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/10best-cars-group-cropped-1542126037.jpg -O cars.jpg\n",
        "  \n",
        "object_detection_api('./cars.jpg', rect_th=6, text_th=5, text_size=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTrjR1xjSh7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://images.unsplash.com/photo-1458169495136-854e4c39548a -O traffic_scene2.jpg\n",
        "  \n",
        "object_detection_api('./traffic_scene2.jpg', rect_th=15, text_th=7, text_size=5, threshold=0.8)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fABPigiMEmo",
        "colab_type": "text"
      },
      "source": [
        "# Comparing the inference time of model in CPU & GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7vkcAYDSqkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def check_inference_time(image_path, gpu=False):\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  model.eval()\n",
        "  img = Image.open(image_path)\n",
        "  transform = T.Compose([T.ToTensor()])\n",
        "  img = transform(img)\n",
        "  if gpu:\n",
        "    model.cuda()\n",
        "    img = img.cuda()\n",
        "  else:\n",
        "    model.cpu()\n",
        "    img = img.cpu()\n",
        "  start_time = time.time()\n",
        "  pred = model([img])\n",
        "  end_time = time.time()\n",
        "  return end_time-start_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DetKUAIeS33o",
        "colab_type": "text"
      },
      "source": [
        "## Inference time for Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6eIc2MjS8yO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cpu_time = sum([check_inference_time('./traffic_scene2.jpg', gpu=False) for _ in range(5)])/5.0\n",
        "gpu_time = sum([check_inference_time('./traffic_scene2.jpg', gpu=True) for _ in range(5)])/5.0\n",
        "\n",
        "\n",
        "print('\\n\\nAverage Time take by the model with GPU = {}s\\nAverage Time take by the model with CPU = {}s'.format(gpu_time, cpu_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqldQXurML0P",
        "colab_type": "text"
      },
      "source": [
        "# Single-stage Object Detection: YOLO & SSD\n",
        "\n",
        "All the techniques we have seen for object-detection till are two-stage, RPN generates object proposals and then Fast RCNN classifies and regress on top of those predicted object proposals. A pertinent question is can we build single stage detectors?\n",
        "\n",
        "### RetinaNet (Focal Loss and FPN combined)](https://arxiv.org/pdf/1708.02002.pdf) as a Single-stage FPN\n",
        "\n",
        "when evaluating 1^⁴ to 1^⁵ anchor boxes in Faster RCNN (FPN) network , most of the boxes do not contain objects, leading to extreme class imbalance. To counter for class imbalance, we are sampling 256 proposal from each mini-batch (128+ve and 128-ve). However this is not a robost approach and the authors of this paper have proposed a loss function called Focal loss which tries to address class imbalance efficiently.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Yolo (You look only once)\n",
        "\n",
        "YOLO is an important framework that has led to improved speed and accuracy for object-detection tasks. Yolo has evolved over a period of time and has published 3 papers till now. Though most of the ideas are similar to the ones we have discussed above, YOLO provides pointed enhancements that especially address faster processing times.\n",
        "\n",
        "*    Yolo has three papers written till to date, Yolo1, yolo2, Yolo3 which showed improvements in terms of accuracy.\n",
        "*    Yolo1 is independently published and to my knowledge is the first paper which talks about single stage detection. RetinaNet takes ideas from Yolo and SSD.\n",
        "\n",
        "\n",
        "\n",
        "![Faster R-CNN](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/ssd-yolo.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uC3Am-DMPDo",
        "colab_type": "text"
      },
      "source": [
        "# YOLOv3\n",
        "\n",
        "![Yolov3](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/yolov3.png)\n",
        "[Yolov3 another description](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/yolov3_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-D1cKBXMUUS",
        "colab_type": "text"
      },
      "source": [
        "# RefineDet & SSD\n",
        "\n",
        "![Faster R-CNN](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/ssd-refinedet.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uousv3HjMXN0",
        "colab_type": "text"
      },
      "source": [
        "# Almost recent benchmark\n",
        "\n",
        "![Faster R-CNN](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/benchmark_2.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYdTkmDAMkPY",
        "colab_type": "text"
      },
      "source": [
        "![Faster R-CNN](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/1*QOGcvHbrDZiCqTG6THIQ_w.png)\n",
        "\n",
        "\n",
        "# Nex step: instance segmentation, MASK-RCNN\n",
        "\n",
        "![Faster R-CNN](https://raw.githubusercontent.com/smajida/DeepLearningWorkshop-ISPRS2019/master/1*vMiMMU6sIfb7aUFXerUIWw.png)\n"
      ]
    }
  ]
}